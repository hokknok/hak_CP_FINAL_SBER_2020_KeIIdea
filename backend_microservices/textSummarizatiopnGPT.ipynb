{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "textSummarizatiopnGPT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U55oJovhKZzK",
        "outputId": "f1ab72b3-b6dc-4271-a616-e216ec966ea6"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py\n",
        "!pip3 install urllib3==1.25.4\n",
        "!pip3 install transformers==2.8.0\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-28 13:09:44--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10474 (10K) [text/plain]\n",
            "Saving to: ‘generate_transformers.py.2’\n",
            "\n",
            "generate_transforme 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-28 13:09:44 (53.8 MB/s) - ‘generate_transformers.py.2’ saved [10474/10474]\n",
            "\n",
            "Requirement already satisfied: urllib3==1.25.4 in /usr/local/lib/python3.6/dist-packages (1.25.4)\n",
            "Requirement already satisfied: transformers==2.8.0 in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.1.94)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.16.25)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.0.43)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.25.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.11.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.20.0,>=1.19.25 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.8.0) (1.19.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.25->boto3->transformers==2.8.0) (2.8.1)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.25.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.11.8)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Bb36nBADKtXf",
        "outputId": "36688e85-c800-4066-fd9c-b224458e5d68"
      },
      "source": [
        "\"\"\"\n",
        "!python generate_transformers.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=sberbank-ai/rugpt3large_based_on_gpt2 \\\n",
        "    --k=5 \\\n",
        "    --p=0.95 \\\n",
        "    --length=100\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n!python generate_transformers.py     --model_type=gpt2     --model_name_or_path=sberbank-ai/rugpt3large_based_on_gpt2     --k=5     --p=0.95     --length=100\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc4vV_uk-YXh"
      },
      "source": [
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from transformers import (\n",
        "    CTRLLMHeadModel,\n",
        "    CTRLTokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    OpenAIGPTLMHeadModel,\n",
        "    OpenAIGPTTokenizer,\n",
        "    TransfoXLLMHeadModel,\n",
        "    TransfoXLTokenizer,\n",
        "    XLMTokenizer,\n",
        "    XLMWithLMHeadModel,\n",
        "    XLNetLMHeadModel,\n",
        "    XLNetTokenizer,\n",
        ")\n",
        "\n",
        "from flask import Flask, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "#logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO,)\n",
        "#logger = logging.getLogger(__name__)\n",
        "\n",
        "MAX_LENGTH = int(10000) \n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    \"gpt2\": (GPT2LMHeadModel, GPT2Tokenizer),\n",
        "    \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n",
        "    \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
        "    \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n",
        "    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
        "    \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n",
        "}\n",
        "\n",
        "def set_seed():\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def prepare_ctrl_input(_, tokenizer, prompt_text):\n",
        "    if temperature > 0.7:\n",
        "        logger.info(\"CTRL typically works better with lower temperatures (and lower top_k).\")\n",
        "\n",
        "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
        "    if not any(encoded_prompt[0] == x for x in tokenizer.control_codes.values()):\n",
        "        logger.info(\"WARNING! You are not starting your generation from a control code so you won't get good results\")\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_xlm_input(model, tokenizer, prompt_text):\n",
        "    use_lang_emb = hasattr(model.config, \"use_lang_emb\") and model.config.use_lang_emb\n",
        "    if hasattr(model.config, \"lang2id\") and use_lang_emb:\n",
        "        available_languages = model.config.lang2id.keys()\n",
        "        if xlm_language in available_languages:\n",
        "            language = xlm_language\n",
        "        else:\n",
        "            language = None\n",
        "            while language not in available_languages:\n",
        "                language = input(\"Using XLM. Select language in \" + str(list(available_languages)) + \" >>> \")\n",
        "\n",
        "        model.config.lang_id = model.config.lang2id[language]\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_xlnet_input(_, tokenizer, prompt_text):\n",
        "    prompt_text = (padding_text if padding_text else PADDING_TEXT) + prompt_text\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "def prepare_transfoxl_input(_, tokenizer, prompt_text):\n",
        "    prompt_text = (padding_text if padding_text else PADDING_TEXT) + prompt_text\n",
        "    return prompt_text\n",
        "\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    \"ctrl\": prepare_ctrl_input,\n",
        "    \"xlm\": prepare_xlm_input,\n",
        "    \"xlnet\": prepare_xlnet_input,\n",
        "    \"transfo-xl\": prepare_transfoxl_input,\n",
        "}\n",
        "\n",
        "\n",
        "def adjust_length_to_model(length, max_sequence_length):\n",
        "    if length < 0 and max_sequence_length > 0:\n",
        "        length = max_sequence_length\n",
        "    elif 0 < max_sequence_length < length:\n",
        "        length = max_sequence_length \n",
        "    elif length < 0:\n",
        "        length = MAX_LENGTH\n",
        "    return length\n",
        "\n",
        "\n",
        "model_type=\"gpt2\"\n",
        "model_name_or_path=\"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
        "k=5\n",
        "p=0.95\n",
        "length=100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "temperature = 1.0\n",
        "seed = 42\n",
        "n_gpu = torch.cuda.device_count()\n",
        "xlm_language = \"\"\n",
        "padding_text = \"\"\n",
        "stop_token = \"</s>\"\n",
        "num_return_sequences = 1\n",
        "repetition_penalty = 1.0\n",
        "\n",
        "\n",
        "model_type = model_type.lower()\n",
        "model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name_or_path)\n",
        "model = model_class.from_pretrained(model_name_or_path)\n",
        "model.to(device)\n",
        "\n",
        "length = adjust_length_to_model(length, max_sequence_length=model.config.max_position_embeddings)\n",
        "#logger.info(args)\n",
        "\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  \n",
        "@app.route(\"/\")\n",
        "def main():\n",
        "\n",
        "    generated_sequences = []\n",
        "    prompt_text = request.args.get(\"prompt_text\")\n",
        "    #prompt_text = \"%20\".join(prompt_text.split(\"%20\")[:20])\n",
        "    print(prompt_text.split(\" \")[:20])\n",
        "\n",
        "    requires_preprocessing = model_type in PREPROCESSING_FUNCTIONS.keys()\n",
        "    if requires_preprocessing:\n",
        "        prepare_input = PREPROCESSING_FUNCTIONS.get(model_type)\n",
        "        preprocessed_prompt_text = prepare_input(model, tokenizer, prompt_text)\n",
        "        encoded_prompt = tokenizer.encode(preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", add_space_before_punct_symbol=True)\n",
        "    else:\n",
        "        encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    encoded_prompt = encoded_prompt.to(device)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "            input_ids=encoded_prompt,\n",
        "            max_length=length + len(encoded_prompt[0]),\n",
        "            temperature=temperature,\n",
        "            top_k=k,\n",
        "            top_p=p,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            do_sample=True,\n",
        "            num_return_sequences=num_return_sequences,\n",
        "        )\n",
        "\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        print(\"ruGPT:\".format(generated_sequence_idx + 1))\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "        text = text[: text.find(stop_token) if stop_token else None]\n",
        "        total_sequence = (prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :])\n",
        "        generated_sequences.append(total_sequence)\n",
        "\n",
        "    return \" \".join(\" \".join(generated_sequences).split()[:50])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}